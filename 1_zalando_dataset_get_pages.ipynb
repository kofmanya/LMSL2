{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48d2ff3-9e61-412c-98a4-0665248870ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import zipfile\n",
    "import shutil\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf7381b-7048-41b6-bb49-0ac58da3abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_ROOT = './'\n",
    "ITEMS_PATH = LOCAL_ROOT + 'data/items.pkl'\n",
    "LOG_PATH = LOCAL_ROOT + 'log.txt'\n",
    "FILE_CACHE_PATH = LOCAL_ROOT + 'data/pages.zip'\n",
    "FILE_CACHE_ALGO = zipfile.ZIP_LZMA\n",
    "CATEGORY_IDS = [\n",
    "    'damenbekleidung-kleider',\n",
    "    'damenbekleidung-shirts',\n",
    "    'damenbekleidung-jeans',\n",
    "    'damenbekleidung-jacken',\n",
    "    'damenbekleidung-jacken-maentel',\n",
    "    'damenbekleidung-blusen-tuniken',\n",
    "]\n",
    "PAGE_LIMIT = 60\n",
    "DRIVER_PATH = '/Users/kofmanya/Desktop/HSE/My assignments/Diploma/Dataset/chromedriver'\n",
    "ZALANDO_URL = 'https://www.zalando.ch'\n",
    "\n",
    "\n",
    "def log(pattern: str, *args):\n",
    "    msg = ('{} ' + pattern + '\\n').format(datetime.now(), *args)\n",
    "    print(msg, end='')\n",
    "    with open(LOG_PATH, 'a', encoding='utf-8') as f:\n",
    "        f.write(msg)\n",
    "\n",
    "\n",
    "class Driver:\n",
    "    def __init__(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        service = Service(DRIVER_PATH)\n",
    "        #self.driver = webdriver.Chrome(executable_path=DRIVER_PATH, options=options)\n",
    "        self.driver = webdriver.Chrome(service=service, options=options)\n",
    "        self.driver.execute_cdp_cmd('Network.enable', {})\n",
    "        self.driver.execute_cdp_cmd('Network.setCookie', {'name': 'language-preference', 'value': 'en', 'domain': 'www.zalando.ch'})\n",
    "        self.driver.execute_cdp_cmd('Network.disable', {})\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def load_complete_page(self, url: str) -> str:\n",
    "        self.driver.get(url)\n",
    "        time.sleep(4)\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        time.sleep(1)\n",
    "        return self.driver.page_source\n",
    "\n",
    "\n",
    "class FileRotator:\n",
    "    def __init__(self, path: str, rotate_interval: float):\n",
    "        self.path = path\n",
    "        self.rotate_interval = rotate_interval\n",
    "        self.last_rotate_ts = time.time()\n",
    "        for suffix in ['.bak4', '.bak3', '.bak2', '.bak1', '']:\n",
    "            if os.path.exists(self.path + suffix):\n",
    "                self.copy_file(suffix, '.old')\n",
    "                break\n",
    "\n",
    "    def before_wrte(self):\n",
    "        ts = time.time()\n",
    "        if ts > self.last_rotate_ts + self.rotate_interval:\n",
    "            log('[info] rotating {}', self.path)\n",
    "            self.last_rotate_ts = ts\n",
    "            self.move_file('.bak3', '.bak4')\n",
    "            self.move_file('.bak2', '.bak3')\n",
    "            self.move_file('.bak1', '.bak2')\n",
    "            self.move_file('', '.bak1')\n",
    "            self.copy_file('.bak1', '')\n",
    "\n",
    "    def copy_file(self, src: str, dst: str):\n",
    "        if os.path.exists(self.path + src):\n",
    "            shutil.copy(self.path + src, self.path + dst)\n",
    "\n",
    "    def move_file(self, src: str, dst: str):\n",
    "        if os.path.exists(self.path + src):\n",
    "            shutil.move(self.path + src, self.path + dst)\n",
    "\n",
    "\n",
    "class FileCache:\n",
    "    def __init__(self):\n",
    "        self.rotator = FileRotator(FILE_CACHE_PATH, 300.0)\n",
    "        try:\n",
    "            log('[info] testing cache integrity')\n",
    "            if os.path.exists(FILE_CACHE_PATH):\n",
    "                with zipfile.ZipFile(FILE_CACHE_PATH, 'r', compression=FILE_CACHE_ALGO, compresslevel=9) as zipf:\n",
    "                    bad_file = zipf.testzip()\n",
    "                    if bad_file:\n",
    "                        log('[error] bad file in cache: {}, please remove it', bad_file)\n",
    "                    else:\n",
    "                        log('[info] cache is ok')\n",
    "            else:\n",
    "                log('[info] cache does not exist')\n",
    "        except:\n",
    "            log('[error] failed to test cache integrity')\n",
    "\n",
    "    def get(self, url: str) -> bytes:\n",
    "        try:\n",
    "            with zipfile.ZipFile(FILE_CACHE_PATH, 'r', compression=FILE_CACHE_ALGO, compresslevel=9) as zipf:\n",
    "                with zipf.open(self.url_to_path(url), 'r') as f:\n",
    "                    return f.read()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def put(self, url: str, data: bytes):\n",
    "        self.rotator.before_wrte()\n",
    "        with zipfile.ZipFile(FILE_CACHE_PATH, 'a', compression=FILE_CACHE_ALGO, compresslevel=9) as zipf:\n",
    "            with zipf.open(self.url_to_path(url), 'w') as f:\n",
    "                f.write(data)\n",
    "\n",
    "    def url_to_path(self, url: str) -> str:\n",
    "        # drop prefix\n",
    "        for prefix in ['http://', 'https://', 'www.']:\n",
    "            if url.startswith(prefix):\n",
    "                url = url[len(prefix):]\n",
    "        # handle slashes\n",
    "        url = url.replace('?', '/')\n",
    "        while '//' in url:\n",
    "            url = url.replace('//', '/')\n",
    "        if url.endswith('/'):\n",
    "            url = url[:-1]\n",
    "        if url.startswith('/'):\n",
    "            url = url[1:]\n",
    "        if len(url) == 0:\n",
    "            raise ValueError('empty url')\n",
    "        # introduce / instead of 4th separator\n",
    "        parts = url.split('/')\n",
    "        for i in range(len(parts)):\n",
    "            for sep in ['-', '_']:\n",
    "                if parts[i].count(sep) >= 4:\n",
    "                    groups = parts[i].split(sep, maxsplit=4)\n",
    "                    parts[i] = sep.join(groups[:-1]) + '/' + groups[-1]\n",
    "                    break\n",
    "        return '/'.join(parts) + '.txt'\n",
    "\n",
    "\n",
    "class Item:\n",
    "    PRICE_CLEANUP_TOKENS = ['&nbsp;', 'From', 'CHF', '\\u2019']\n",
    "    PRICE_ATTRS = [\n",
    "        'span class=\"voFjEy _4sa1cA m3OCL3 HlZ_Tf _65i7kZ\"',\n",
    "        'span class=\"voFjEy _4sa1cA m3OCL3 Yb63TQ ZiDB59 _65i7kZ\"',\n",
    "    ]\n",
    "\n",
    "    def __init__(self, category_id: str, category_page: int, item_id: str, url: str, html: str):\n",
    "        self.category = category_id\n",
    "        self.category_page = category_page\n",
    "        self.id = item_id\n",
    "        self.url = url\n",
    "        self.images = Item.find_images(html)\n",
    "        self.brand = Item.find_brand(html)\n",
    "        self.name = Item.find_name(html)\n",
    "        self.price = Item.find_price(html)\n",
    "        self.color = Item.find_color(html)\n",
    "        self.material = Item.find_material(html)\n",
    "        error, warning = self.check_fields()\n",
    "        if error:\n",
    "            self.error = error\n",
    "            log('[error] {} - {}', url, error)\n",
    "        elif warning:\n",
    "            self.warning = warning\n",
    "            log('[warning] {} - {}', url, warning)\n",
    "        else:\n",
    "            log('[debug] {} | {} | {} | {} | {} | {} | {} img(s)', self.url, self.brand, self.name, self.price, self.color, self.material, len(self.images))\n",
    "            pass\n",
    "\n",
    "\n",
    "    def find_images(html: str) -> list[str]:\n",
    "        images1 = []\n",
    "        for match in re.finditer('\"Thumbnail Image .\" src=\"(https://img01.ztat.net/article/[^\"]+\\.[a-zA-Z]{3,4})', html):\n",
    "            images1.append(match.group(1))\n",
    "        images2 = []\n",
    "        for match in re.finditer(', Enlarge\"( fetchpriority=\"[a-z]+\")? src=\"(https://img01.ztat.net/article/[^\"]+\\.[a-zA-Z]{3,4})', html):\n",
    "            images2.append(match.group(2))\n",
    "        # merge 2 lists preserving order and removing duplicates\n",
    "        res = []\n",
    "        for i in range(max(len(images1), len(images2))):\n",
    "            for x in [images1, images2]:\n",
    "                if i < len(x) and (x[i] not in res):\n",
    "                    res.append(x[i])\n",
    "        return res\n",
    "\n",
    "    def find_brand(html: str) -> str:\n",
    "        return Item.find_attr(html, 'h5 class=\"voFjEy YZziZ- m3OCL3 HlZ_Tf q84f1m snL7ze\"')\n",
    "    \n",
    "    def find_name(html: str) -> str:\n",
    "        return Item.find_attr(html, 'span class=\"EKabf7 R_QwOV\"')\n",
    "\n",
    "    def find_price(html: str) -> str:\n",
    "        for attr in Item.PRICE_ATTRS:\n",
    "            prices = Item.find_attrs(html, attr, 2)\n",
    "            for price in prices:\n",
    "                price = Item.cleanup_price(price)\n",
    "                if price:\n",
    "                    return price\n",
    "        return ''\n",
    "    \n",
    "    def cleanup_price(price: str) -> str:\n",
    "        try:\n",
    "            for token in Item.PRICE_CLEANUP_TOKENS:\n",
    "                price = price.replace(token, '')\n",
    "            price.strip()\n",
    "            if price:\n",
    "                return str(float(price))\n",
    "        except:\n",
    "            pass\n",
    "        return ''            \n",
    "\n",
    "    def find_color(html: str) -> str:\n",
    "        return Item.find_attr(html, 'span class=\"voFjEy lystZ1 Sb5G3D HlZ_Tf zN9KaA\"')\n",
    "\n",
    "    def find_material(html: str) -> str:\n",
    "        return Item.find_attr(html, 'dd class=\"voFjEy lystZ1 m3OCL3 HlZ_Tf zN9KaA\" role=\"definition\"')\n",
    "    \n",
    "    def find_attrs(html: str, attr: str, limit: int) -> list[str]:\n",
    "        res = []\n",
    "        start = 0\n",
    "        for i in range(limit):\n",
    "            start = html.find(attr + '>', start)\n",
    "            if start < 0:\n",
    "                break\n",
    "            end = html.find('</', start + len(attr) + 1)\n",
    "            if end < 0:\n",
    "                break\n",
    "            res.append(html[start + len(attr) + 1:end])\n",
    "            start = end\n",
    "        return res\n",
    "\n",
    "    def find_attr(html: str, attr: str) -> str:\n",
    "        start = html.find(attr + '>')\n",
    "        if start >= 0:\n",
    "            end = html.find('</', start + len(attr) + 1)\n",
    "            if end >= 0:\n",
    "                return html[start + len(attr) + 1:end]\n",
    "        return ''\n",
    "\n",
    "    def check_fields(self) -> tuple[str, str]:\n",
    "        fields = self.__dict__\n",
    "        for field in ['brand', 'name', 'price', 'color', 'material', 'images']:\n",
    "            if not fields[field]:\n",
    "                return ('no {}'.format(field), '')\n",
    "        if len(self.images) > 0 and len(self.images) < 3:\n",
    "            return ('', 'only {} image(s)'.format(len(self.images)))\n",
    "        for image in self.images:\n",
    "            dot = image.rfind('.')\n",
    "            if dot >= 0 and (image[dot:].lower() != '.jpg'):\n",
    "                return ('', 'non-jpg image {}'.format(image[dot:]))\n",
    "        return ('', '')\n",
    "\n",
    "\n",
    "class ItemScraper:\n",
    "    def __init__(self, driver: Driver):\n",
    "        self.driver = driver\n",
    "        self.cache = FileCache()\n",
    "        self.items_dict_rotator = FileRotator(ITEMS_PATH, 60)\n",
    "        self.items_dict = dict()\n",
    "        if os.path.exists(ITEMS_PATH):\n",
    "            with open(ITEMS_PATH, 'rb') as f:\n",
    "                self.items_dict = pickle.load(f)\n",
    " \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.save_items()\n",
    "\n",
    "    def save_items(self):\n",
    "        self.items_dict_rotator.before_wrte()\n",
    "        with open(ITEMS_PATH, 'wb') as f:\n",
    "            pickle.dump(self.items_dict, f)\n",
    "        log('[info] dumped {} items', len(self.items_dict))\n",
    "\n",
    "    def scrape_category(self, category_id: str):\n",
    "        category_url = '{}/{}/'.format(ZALANDO_URL, category_id)\n",
    "        item_ids = self.get_item_ids_in_category(category_url)\n",
    "        for item_id, category_page in item_ids.items():\n",
    "            if item_id in self.items_dict:\n",
    "                continue\n",
    "            item = self.load_item(category_id, category_page, item_id)\n",
    "            self.items_dict[item_id] = item.__dict__\n",
    "            if len(self.items_dict) % 100 == 0:\n",
    "                self.save_items()\n",
    "        self.save_items()\n",
    "\n",
    "    def get_item_ids_in_category(self, category_url: str) -> dict[str, int]:\n",
    "        item_ids = dict()\n",
    "        num_pages = self.get_num_pages_in_category(category_url)\n",
    "        log('[info] category {} has {} pages', category_url, num_pages)\n",
    "        for i in range(1, min(num_pages, PAGE_LIMIT) + 1):\n",
    "            url = '{}?p={}'.format(category_url, i)\n",
    "            html = self.load_complete_page(url)\n",
    "            on_page = set()\n",
    "            for match in re.finditer('href=\"https://www.zalando.ch/([^\"]+)\\.html\"', html):\n",
    "                on_page.add(match.group(1))\n",
    "            log_level = '[warning]' if len(on_page) < 60 else '[debug]'\n",
    "            log('{} page {}?p={} has {} items', log_level, category_url, i, len(on_page))\n",
    "            for item_id in on_page:\n",
    "                item_ids[item_id] = i\n",
    "        log('[info] category {} has {} items', category_url, len(item_ids))\n",
    "        return item_ids\n",
    "\n",
    "    def get_num_pages_in_category(self, category_url: str) -> int:\n",
    "        html = self.load_complete_page(category_url)\n",
    "        begin_pos = html.find('>Page 1 of ')\n",
    "        end_pos = html.find('</span>', begin_pos)\n",
    "        return int(html[begin_pos + 11:end_pos])\n",
    "\n",
    "    def load_item(self, category_id: str, category_page: int, item_id: str) -> Item:\n",
    "        url = '{}/{}.html'.format(ZALANDO_URL, item_id)\n",
    "        html = self.load_complete_page(url)\n",
    "        return Item(category_id, category_page, item_id, url, html)\n",
    "\n",
    "    def load_complete_page(self, url: str) -> str:\n",
    "        cached = self.cache.get(url)\n",
    "        if cached:\n",
    "            return cached.decode('utf-8')\n",
    "        html = self.driver.load_complete_page(url)\n",
    "        self.cache.put(url, html.encode('utf-8'))\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d7bb4-e290-4e26-a49f-112ecf144365",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Driver() as driver:\n",
    "    scrapper = ItemScraper(driver)\n",
    "    for category_id in CATEGORY_IDS:\n",
    "        scrapper.scrape_category(category_id)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f39a5c-6a4b-41f2-ade5-ab1ac76b245a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
